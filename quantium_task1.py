# -*- coding: utf-8 -*-
"""Quantium_Task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14IaKY4aK1taXbwFH05Xq1UNxt1Dl_3in

**REQUIREMENTS**
*   Examine transaction data – look for inconsistencies, missing data across the data set, outliers, correctly identified category items, numeric data across all tables. If you determine any anomalies make the necessary changes in the dataset and save it. Having clean data will help when it comes to your analysis. 

*   Examine customer data – check for similar issues in the customer data, look for nulls and when you are happy merge the transaction and customer data together so it’s ready for the analysis ensuring you save your files along the way.

*   Data analysis and customer segments – in your analysis make sure you define the metrics – look at total sales, drivers of sales, where the highest sales are coming from etc. Explore the data, create charts and graphs as well as noting any interesting trends and/or insights you find. 

*   Deep dive into customer segments – define your recommendation from your insights, determine which segments we should be targeting, if packet sizes are relative and form an overall conclusion based on your analysis. 

*   Save your analysis in the CSV files and your visualisations
"""

from google.colab import drive
drive.mount('/content/drive')

#Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Reading the data
transaction_data=pd.read_excel('/content/drive/MyDrive/QVI_transaction_data.xlsx')
transaction_data.head()

"""## **EXAMINING TRANSACTION DATA**"""

transaction_data.info()

"""***DATA WRANGLING***"""

#The date column is inconsistent as it is stored in integer format.
#We create a function to convert it to date

import xlrd
import datetime
def xldate_to_datetime(xlserialdate):
  anchor=datetime.datetime(1900,1,1)
  if (xlserialdate < 60):
    delta=datetime.timedelta(days=(xlserialdate - 1))
  else:
    delta=datetime.timedelta(days=(xlserialdate - 2))
  converted_date= anchor + delta
  return converted_date

#Apply the date function to the date column
transaction_data['DATE']=transaction_data['DATE'].apply(xldate_to_datetime)

transaction_data['DATE'].head()

transaction_data.head()

#Examining shape of the transaction data
transaction_data.shape

#Examining number of unique date in date entry
transaction_data['DATE'].nunique()

#There are 365 days in a year so we check for the specific missing date
pd.date_range(start = '2018-07-01',end='2019-06-30').difference(transaction_data['DATE'])

transaction_data['PROD_NAME'].head()

#Creating a new column by extracting size of pack from 'PROD_NAME'
transaction_data['SIZE_OF_PACK']=transaction_data['PROD_NAME'].str.extract('(\d+)')
transaction_data['SIZE_OF_PACK'] = pd.to_numeric(transaction_data['SIZE_OF_PACK'])
transaction_data.head()

#Creating a function that removes metadata as well as size of pack from 'PROD_NAME'
import re

def clean_txt(txt):
  txt=re.sub('[&/]','',txt)
  txt=re.sub('\d\w*','',txt)
  return txt

transaction_data['PROD_NAME']=transaction_data['PROD_NAME'].apply(clean_txt)

transaction_data['PROD_NAME'].head()

transaction_data.head()

#Dropping 'Salsa' rows in 'PROD_NAME'
transaction_data['PROD_NAME']=transaction_data['PROD_NAME'].apply(lambda x:x.lower())
transaction_data=transaction_data[~transaction_data['PROD_NAME'].str.contains('salsa')]
transaction_data['PROD_NAME']=transaction_data['PROD_NAME'].apply(lambda x:x.title())

#Checking for outliers in 'PROD_QTY'
transaction_data['PROD_QTY'].value_counts()

#Examining the 200 product quantity bought
transaction_data.loc[transaction_data['PROD_QTY'] == 200,: ]

#Examining the 200 product quantity bought by 'LYLTY_CARD_NBR'
transaction_data.loc[transaction_data['LYLTY_CARD_NBR'] == 226000, :]

#Since the customer has made large purchases twice in a span of ten months it is safe to conclude he/she is not a retail customer.It is likely they bought it for commercial purposes so we drop the entry from the dataset.
transaction_data.drop(transaction_data.index[transaction_data['LYLTY_CARD_NBR'] == 226000],inplace=True)

#Checking to see if the entry has been dropped as is is invalid in our analysis
transaction_data.loc[transaction_data['LYLTY_CARD_NBR'] == 226000]

#Extracting and creating a brand name from 'PROD_NAME'
name=transaction_data['PROD_NAME'].str.partition()
transaction_data['BRAND'] = name[0]
transaction_data.head()

#Identifying unique brand names
transaction_data['BRAND'].unique()

#Some of the brand names are inconsistent in how they are named like 'Woolworths'and 'Ww'
#We rename them into one name for pupose of consistency in the data
transaction_data['BRAND'].replace('Natural','Ncc',inplace=True)
transaction_data['BRAND'].replace('Smith','Smiths',inplace=True)
transaction_data['BRAND'].replace('Ww','Woolworths',inplace=True)
transaction_data['BRAND'].replace('Infzns','Infuzions',inplace=True)
transaction_data['BRAND'].replace('Snbts','Sunbites',inplace=True)
transaction_data['BRAND'].replace(['Grain','Grnwves'],'Grainwaves',inplace=True)
transaction_data['BRAND'].replace(['Rrd','Red'],'Red Rock Deli',inplace=True)
transaction_data['BRAND'].replace('Dorito','Doritos',inplace=True)

transaction_data['BRAND'].unique()

transaction_data.groupby('BRAND').TOT_SALES.sum().sort_values(ascending=False)

#Kettle makes the highest sales from chips while Burger makes the lowest

transaction_data.groupby('BRAND').SIZE_OF_PACK.max().sort_values(ascending=False)

#We can see that Smith and Doritos sell the largest pack sizes of chips while Sunbites sells the smallest size of chips

#Creating a pivot table of total sales and date
sales_by_date=pd.pivot_table(transaction_data,values='TOT_SALES',index='DATE',aggfunc='sum')
sales_by_date.head()

#Creating a dataframe from the pivot table created
sales_by_date_df=pd.DataFrame(index=pd.date_range(start = '2018-07-01',end='2019-06-30'))
sales_by_date_df['TOT_SALES'] = sales_by_date
sales_by_date_df

"""***DATA VISUALISATION***"""

#Total Sales by date plot

sales_by_date_df.plot()

#Checking for unique entrie in 'SIZE_OF_PACK'
transaction_data['SIZE_OF_PACK'].unique()

#Histogram visualisation of size of packs sold
transaction_data['SIZE_OF_PACK'].hist()

"""**CUSTOMER DATA**"""

#Reading customer data
purchase_behaviour=pd.read_csv('/content/drive/MyDrive/QVI_purchase_behaviour.csv')
purchase_behaviour.head()

purchase_behaviour.info()

#Examining shape of customer data
purchase_behaviour.shape

#Removing the entry with outliers
purchase_behaviour.drop(purchase_behaviour.index[purchase_behaviour['LYLTY_CARD_NBR'] == 226000],inplace=True)
purchase_behaviour.loc[['LYLTY_CARD_NBR']== 226000:]

#Checking for duplicate entries in 'LYLTY_CARD_NUMBER'
purchase_behaviour['LYLTY_CARD_NBR'].nunique()

#Checking for duplicate entries in 'LIFESTAGE'
purchase_behaviour['LIFESTAGE'].unique()

#Checking for count of each lifestage
purchase_behaviour['LIFESTAGE'].value_counts().sort_values(ascending=False)

"""***DATA VISUALISATION***"""

#Visualisation of lifestage by count
sns.countplot(y=purchase_behaviour['LIFESTAGE'],order=purchase_behaviour['LIFESTAGE'].value_counts().index)

purchase_behaviour['PREMIUM_CUSTOMER'].unique()

#Visualisation of 'PREMIUM_CUSTOMER'
sns.countplot(y=purchase_behaviour['PREMIUM_CUSTOMER'])

"""**MERGING TRANSACTION DATA WITH CUSTOMER DATA**"""

#Since transaction data and customer data have a column similar to each other i.e. 'LYLTY_CARD_NBR' we can merge them.


customer_transactions=pd.merge(transaction_data,purchase_behaviour)
customer_transactions.head()

customer_transactions.info()

"""**CUSTOMER SEGMENTATION ANALYSIS**

> Having merged our datasets we can derive as well  as visualise some information such as:

- Who spends the most on chips (total sales), describing customers by lifestage and how premium their general purchasing behaviour is
- How many customers are in each segment
- How many chips are bought per customer by segment
- What's the average chip price by customer segment





"""

#'TOT_SALES' by 'LIFESTAGE' and 'PREMIUM_CUSTOMER'
Sales_Lifestage_Premium=pd.DataFrame(customer_transactions.groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).TOT_SALES.sum())
Sales_Lifestage_Premium.sort_values(by='TOT_SALES',ascending=False)

#Visualisation of Sales_Lifestage_Premium
Sales_Lifestage_Premium.unstack().plot(kind='bar',stacked=False,figsize=(10,5),title='Total Sales')
plt.ylabel('Total Sales')

#Are higher sales generated by more customers
customers_per_segment=pd.DataFrame(customer_transactions.groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).LYLTY_CARD_NBR.nunique())
customers_per_segment.sort_values(by='LYLTY_CARD_NBR',ascending=False)

#Visualisation of Customer_per_segment
customers_per_segment.unstack().plot(kind='bar',stacked=False,figsize=(12,7),title='Customers per segment')
plt.ylabel('Number of Customers')

#Chips bought by a customer on average
chip_avg_unit=customer_transactions.groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).PROD_QTY.sum() / customer_transactions.groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).LYLTY_CARD_NBR.nunique()
chip_avg_unit.sort_values(ascending=False)

#Visualisation of average chips bought by a customer segment

chip_avg_unit.unstack().plot(kind='bar',stacked=False,figsize=(12,7),title='Average chip per customer segment')
plt.ylabel('Average chips bought by customers in each segment')

#Average price of chips by customer
avg_chip_price=customer_transactions.groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).TOT_SALES.sum() / customer_transactions.groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).PROD_QTY.sum()
avg_chip_price.sort_values(ascending=False)

#Visualisation of amount spent on a packet of chips by a customer segment
avg_chip_price.unstack().plot(kind='bar',stacked=False,figsize=(12,7),title='Amount spent per chips')
plt.ylabel('Price per chips')

#Creating a new column for price per packet
PRICE_PER_PKT=customer_transactions
customer_transactions['PRICE_PER_PKT']=customer_transactions['TOT_SALES'] / customer_transactions['PROD_QTY']
customer_transactions.head()

#Mainstream and Non_Mainstream segmentation
Mainstream=PRICE_PER_PKT.loc[(PRICE_PER_PKT['PREMIUM_CUSTOMER'] == 'Mainstream') & ((PRICE_PER_PKT['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES') | (PRICE_PER_PKT['LIFESTAGE'] == 'MIDAGE SINGLES/COUPLES')), 'PRICE_PER_PKT']
Non_Mainstream=PRICE_PER_PKT.loc[(PRICE_PER_PKT['PREMIUM_CUSTOMER'] != 'Mainstream') & ((PRICE_PER_PKT['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES') | (PRICE_PER_PKT['LIFESTAGE'] == 'MIDAGE SINGLES/COUPLES')), 'PRICE_PER_PKT']

np.mean(Mainstream)

np.mean(Non_Mainstream)

#Perform an independent t-test between mainstream vs non-mainstream midage and young singles/couples to test the significant difference in price
from scipy.stats import ttest_ind
ttest_ind(Mainstream,Non_Mainstream)

#Assigning target groups to Mainstream Young Singles/Couples against Budget & Premium counterparts
target_group= customer_transactions.loc[(customer_transactions['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES') & (customer_transactions['PREMIUM_CUSTOMER'] == 'Mainstream'), :]
non_target_group=customer_transactions.loc[(customer_transactions['LIFESTAGE'] != 'YOUNG SINGLES/COUPLES') & (customer_transactions['PREMIUM_CUSTOMER'] != 'Mainstream'), :]
target_group.head()

"""**BRAND PREFERENCE**"""

#Establishing brand preference of MainStream Young Singles/Couples against Budget & Premium counterparts
#Target Segment
targetBrand = target_group.loc[:, ['BRAND', 'PROD_QTY']]
targetSum = targetBrand['PROD_QTY'].sum()
targetBrand['Target Brand Preference'] = targetBrand['PROD_QTY'] / targetSum
targetBrand = pd.DataFrame(targetBrand.groupby('BRAND')['Target Brand Preference'].sum())
targetBrand.head()

#non_target_group
nonTargetBrand = non_target_group.loc[:, ['BRAND', 'PROD_QTY']]
nonTargetSum = nonTargetBrand['PROD_QTY'].sum()
nonTargetBrand['Non-Target Brand Preference'] = nonTargetBrand['PROD_QTY'] / nonTargetSum
nonTargetBrand = pd.DataFrame(nonTargetBrand.groupby('BRAND')['Non-Target Brand Preference'].sum())
nonTargetBrand.head()

#Merging the target and non target groups
brand_preferences=pd.merge(targetBrand,nonTargetBrand,left_index=True,right_index=True)
brand_preferences.head()

brand_preferences['Preference to Brand'] = brand_preferences['Target Brand Preference'] / brand_preferences['Non-Target Brand Preference']
brand_preferences.sort_values(by = 'Preference to Brand', ascending = False)

"""**PACKET_SIZE PREFERENCE**"""

#target group
targetSize = target_group.loc[:, ['SIZE_OF_PACK', 'PROD_QTY']]
targetSum = targetSize['PROD_QTY'].sum()
targetSize['Target Size Preference'] = targetSize['PROD_QTY'] / targetSum
targetSize= pd.DataFrame(targetSize.groupby('SIZE_OF_PACK')['Target Size Preference'].sum())
targetSize.head()

#non_target group
nontargetSize = non_target_group.loc[:, ['SIZE_OF_PACK', 'PROD_QTY']]
nontargetSum = nontargetSize['PROD_QTY'].sum()
nontargetSize['Non_Target Size Preference'] = nontargetSize['PROD_QTY'] / nontargetSum
nontargetSize= pd.DataFrame(nontargetSize.groupby('SIZE_OF_PACK')['Non_Target Size Preference'].sum())
nontargetSize.head()

packet_size=pd.merge(targetSize,nontargetSize,left_index=True,right_index=True)
packet_size.head()

packet_size['Packsize Preference'] = packet_size['Target Size Preference'] / packet_size['Non_Target Size Preference']
packet_size.sort_values(by = 'Packsize Preference', ascending = False)

customer_transactions.loc[customer_transactions['SIZE_OF_PACK'] == 270,:]

customer_transactions.loc[customer_transactions['SIZE_OF_PACK'] == 270, 'BRAND'].unique()



"""**SUMMARY**


*   Budget older families, mainstream young singles/couples and premium older singles/couples contribute to a majority of sales

* Mainstream young singles/couples are the largest group with the highest number of customers while Premium new families are the smallest group with lowest number of customers

*   On average,older families and younger families purchase more chips
Mainstream midage and young singles/couples are willing to pay more per packet of chips 

* The group that should be targeted to boost even more sales is the young singles/couples who prefer Tyrells brand and have affinity to the 270g packet size of chips

* Twisties is the only brand that produces 270g packet size of chips




"""